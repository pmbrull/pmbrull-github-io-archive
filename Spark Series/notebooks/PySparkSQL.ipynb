{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Brull BorrÃ s, Pere Miquel**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the extra information given to spark by the interface of SparkSQL about the internal data schema and the computation, Spark can further optimize the data processing. Thus, RDDs evolved into a new type of data structure: *Datasets*, which benefit from the engine provided by SparkSQL. If their information is structured, we will call them *DataFrames*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Querying structured data with SparkSQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataFrames can be created from external data sources, results of queries or regular RDDs. However, as we just said, they need structured information, i.e an **schema**. What if the data source is encoded in a string or it is the result of parsing a text? One will need to follow these steps:\n",
    "\n",
    "1. Create an RDD of tuples or lists from the original RDD;\n",
    "2. Create the schema represented by a StructType matching the structure of tuples or lists specified before.\n",
    "3. Apply the schema to the RDD via createDataFrame method.\n",
    "\n",
    "Our schema will have the following fields:\n",
    "\n",
    "- type\n",
    "- region\n",
    "- alc\n",
    "- m_acid\n",
    "- ash\n",
    "- alc_ash\n",
    "- mgn\n",
    "- t_phenols\n",
    "- flav\n",
    "- nonflav_phenols\n",
    "- proant\n",
    "- col\n",
    "- hue\n",
    "- od280od315\n",
    "- proline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|  type|\n",
      "+------+\n",
      "|type_1|\n",
      "|type_3|\n",
      "|type_2|\n",
      "|type_1|\n",
      "|type_2|\n",
      "+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "from time import time\n",
    "import math\n",
    "\n",
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "# Load File\n",
    "lines = sc.textFile(\"../input/Wines/wines10m.txt\")\n",
    "wines = lines.map(lambda l: l.split(','))\n",
    "\n",
    "# Schema String\n",
    "schemaString = \"type region alc m_acid ash alc_ash mgn t_phenols flav nonflav_phenols proant col hue od280od315 proline\"\n",
    "\n",
    "fields = [StructField(field_name, StringType(), True) for field_name in schemaString.split()]\n",
    "schema = StructType(fields)\n",
    "\n",
    "# Apply the schema to the RDD\n",
    "wines_df = sqlContext.createDataFrame(wines, schema)\n",
    "\n",
    "# Creates a temporary view using the DataFrame\n",
    "wines_df.registerTempTable(\"wines\")\n",
    "\n",
    "# SQL can be run over DataFrames that have been registered as a table.\n",
    "results = spark.sql(\"SELECT type FROM wines\")\n",
    "\n",
    "results.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to get the number of *regions* that have *alc* greater than 11, and grouped by *type*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|  type|count|\n",
      "+------+-----+\n",
      "|type_3|16334|\n",
      "|type_2|16449|\n",
      "|type_1|16520|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Using direct SQL query:\n",
    "results = spark.sql(\"SELECT type, COUNT(region) as count FROM wines WHERE alc > 11 GROUP BY type\")\n",
    "results.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|  type|count|\n",
      "+------+-----+\n",
      "|type_3|16334|\n",
      "|type_2|16449|\n",
      "|type_1|16520|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Using queries as DataFrame operations:\n",
    "wines_df.select(\"type\",\"region\").filter(wines_df.alc > 11).groupBy(\"type\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For getting descriptive statistics of the column *proline*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|           proline|\n",
      "+-------+------------------+\n",
      "|  count|             82278|\n",
      "|   mean|1051.0387899888356|\n",
      "| stddev| 547.7868318908558|\n",
      "|    min|        100.019139|\n",
      "|    max|        999.996608|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wines_df.describe('proline').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining SparkSQL with RDDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a list of regions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 50 different region names and 1000 different regions.\n"
     ]
    }
   ],
   "source": [
    "regions = [\"Albania\", \"Andorra\", \"Armenia\", \"Austria\", \"Azerbaijan\",\n",
    "        \"Belarus\", \"Belgium\", \"Bosnia and Herzegovina\", \"Bulgaria\", \"Croatia\", \"Cyprus\", \"Czech Republic\",\n",
    "        \"Denmark\", \"Estonia\", \"Finland\", \"France\", \"Georgia\", \"Germany\", \"Greece\", \"Hungary\", \"Iceland\", \"Ireland\",\n",
    "        \"Italy\", \"Kosovo\", \"Latvia\", \"Liechtenstein\", \"Lithuania\", \"Luxembourg\", \"Macedonia\", \"Malta\", \"Moldova\",\n",
    "        \"Monaco\", \"Montenegro\", \"Netherlands\", \"Norway\", \"Poland\", \"Portugal\", \"Romania\", \"Russia\", \"San Marino\",\n",
    "        \"Serbia\", \"Slovakia\", \"Slovenia\", \"Spain\", \"Sweden\", \"Switzerland\", \"Turkey\", \"Ukraine\", \"United Kingdom\",\n",
    "        \"Vatican City (Holy See)\"]\n",
    "\n",
    "print('There are {} different region names and {} different regions.'.format(len(regions), \n",
    "                                                                             wines_df.select('region').distinct().count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpret the region ID modulo 50 as the index in the list of names. Then run the query:\n",
    "- Region name and average hue, grouped by region."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train.groupby('Age').agg({'Purchase': 'mean'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|region_name|index|\n",
      "+-----------+-----+\n",
      "|    Albania|    0|\n",
      "|    Andorra|    1|\n",
      "|    Armenia|    2|\n",
      "|    Austria|    3|\n",
      "| Azerbaijan|    4|\n",
      "+-----------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# zipWithIndex returns a tuple of (item,index)\n",
    "region_df = sc.parallelize(regions).zipWithIndex().toDF(['region_name','index'])\n",
    "region_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------+\n",
      "|region_name|          avg(hue)|\n",
      "+-----------+------------------+\n",
      "|     Russia|1.0274380208333334|\n",
      "|     Sweden|1.0110698878205127|\n",
      "|     Turkey|1.0150838590163933|\n",
      "|    Germany|0.9576503299663294|\n",
      "|     France|1.0293217873754166|\n",
      "|     Greece|1.0022269190140851|\n",
      "|     Kosovo|0.9908144545454549|\n",
      "|   Slovakia|1.0189964965034963|\n",
      "|    Belgium|1.0121928506016018|\n",
      "| San Marino|1.0132053802395218|\n",
      "|    Albania|0.9868996964463652|\n",
      "|    Finland|  1.00433427090301|\n",
      "|    Belarus|0.9912486173402886|\n",
      "|      Malta|0.9995766026490073|\n",
      "|    Croatia|0.9953677453642413|\n",
      "|    Andorra| 0.999903643186308|\n",
      "|      Italy|1.0135391445783137|\n",
      "|  Lithuania|0.9720043536977491|\n",
      "|     Norway|0.9961161693290737|\n",
      "|      Spain|1.0206541597444092|\n",
      "+-----------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Join DataFrames\n",
    "wines_names_df = wines_df.join(region_df, wines_df[\"region\"]==region_df[\"index\"]%50, 'inner').drop(\"index\")\n",
    "\n",
    "wines_names_df.select(\"region_name\", \"hue\").groupBy(\"region_name\").agg({'hue': 'mean'}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check SparkSQL documentation in this [link](http://spark.apache.org/docs/latest/sql-programming-guide.html#programmatically-specifying-the-schema), which is the information source."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
