{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Risk of cancer using KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we are going to use Pyspark to implement the classification of breast tumors as malign or bening, performing predictions with the simplest version of the K Nearest Neighbour: 1NN. We will follow this guidelines:\n",
    "\n",
    "1. Split the train and test sets.\n",
    "2. Generate all possible combinations: cartesian product.\n",
    "3. For each combination $\\{train_i,test_j\\} \\forall i,j$, calculate Euclidian distance of the predictor variables.\n",
    "4. For each test instance, predict with the observation of the training set with minimum distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file has 570 lines.\n"
     ]
    }
   ],
   "source": [
    "cancerRDD = sc.textFile(\"../input/Tumor/breast.csv\")\n",
    "print('The file has {} lines.'.format(cancerRDD.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "header = cancerRDD.first()\n",
    "predictors = [x for x in header.split(',') if x not in ['id','diagnosis','train']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mradius', 'mtexture', 'mperimeter', 'marea', 'msmooth', 'mcompact', 'mconcavity', 'mconc_p', 'msymmetry', 'mfractal', 'sradius', 'stexture', 'sperimeter', 'sarea', 'ssmooth', 'scompact', 'sconcavity', 'sconc_p', 'ssymmetry', 'sfractal', 'lradius', 'ltexture', 'lperimeter', 'larea', 'lsmooth', 'lcompact', 'lconcavity', 'lconc_p', 'lsymmetry', 'lfractal']\n"
     ]
    }
   ],
   "source": [
    "print(predictors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to create two RDD's from this one, so we will cache the result of splitting the lines in order to not reading from disk twice. Train/test distinction is in the last position:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cartesian product has 65440 lines.\n"
     ]
    }
   ],
   "source": [
    "splitRDD = cancerRDD.filter(lambda line: line != header)\\\n",
    "            .map(lambda line: line.split(','))\\\n",
    "            .cache()\n",
    "        \n",
    "trainRDD = splitRDD.filter(lambda f: f[32] == '1')\n",
    "testRDD = splitRDD.filter(lambda f: f[32] == '0')\n",
    "\n",
    "fullRDD = testRDD.cartesian(trainRDD)\n",
    "print('The cartesian product has {} lines.'.format(fullRDD.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have tuples {key,value} where keys are the test instances and values each train observation. In order to get the minimum distance we need to group all values together for each key, calculate the distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pred = len(predictors)\n",
    "\n",
    "def calc_dist(Tuple):\n",
    "    train = Tuple[1]\n",
    "    test = Tuple[0]\n",
    "    dist = 0.0\n",
    "    for i in range(2,32):\n",
    "        dist = dist + (float(train[i])-float(test[i]))**2\n",
    "    return (test[0],((dist)**0.5, train[1]+'_'+test[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function  <span style=\"color:blue\">calc_dist</span> returns, given an input tuple {key=test,value=train}, another tuple with key the test *id* and value the distance calculated using the predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('842302', (341.7302620944424, 'M_M')),\n",
       " ('842302', (376.45576487702664, 'M_M')),\n",
       " ('842302', (538.0234879671779, 'M_M')),\n",
       " ('842302', (1389.4612365464616, 'M_M')),\n",
       " ('842302', (1206.344557277833, 'M_M'))]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distRDD = fullRDD.map(calc_dist)\n",
    "distRDD.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's operate over every value of each key and keep the tuple {distance,prediction} that has the minimum value. Then, readjust the key, making it the X_Y value and compute the count of each distinct one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The final prediction set consists of:\n",
      "('B_B', 94)\n",
      "('B_M', 5)\n",
      "('M_B', 4)\n",
      "('M_M', 57)\n"
     ]
    }
   ],
   "source": [
    "pred = distRDD.reduceByKey(lambda val1, val2: val1 if val1[0]<val2[0] else val2)\\\n",
    "            .mapValues(lambda value: value[1])\\\n",
    "            .values()\\\n",
    "            .map(lambda x: (x,1))\\\n",
    "            .reduceByKey(lambda val1, val2: val1 + val2)\\\n",
    "            .sortByKey()\\\n",
    "            .collect()\n",
    "\n",
    "print('The final prediction set consists of:')\n",
    "for elem in pred:\n",
    "    print(elem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Obtain a list of tuples {distance, prediction} for each *id*: groupByKey.\n",
    "- Sort the list by the first value and keep the first K tuples: distance: mapValues + sorted + [:K]\n",
    "- Convert list of tuples {dist,pred} to list of {pred}. \n",
    "- Get most common element in list.\n",
    "- Again, readjust new key to the prediction and compute the counting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The final prediction set consists of:\n",
      "('B_B', 95)\n",
      "('B_M', 4)\n",
      "('M_B', 3)\n",
      "('M_M', 58)\n"
     ]
    }
   ],
   "source": [
    "pred = distRDD.groupByKey()\\\n",
    "            .mapValues(lambda vec: sorted(vec, key=lambda x: x[0])[:11])\\\n",
    "            .mapValues(lambda vec: [x[1] for x in vec])\\\n",
    "            .mapValues(lambda vec: Counter(vec).most_common(1)[0][0])\\\n",
    "            .values()\\\n",
    "            .map(lambda x: (x,1))\\\n",
    "            .reduceByKey(lambda val1, val2: val1 + val2)\\\n",
    "            .sortByKey()\\\n",
    "            .collect()\n",
    "\n",
    "print('The final prediction set consists of:')\n",
    "for elem in pred:\n",
    "    print(elem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With K=11 we get a small improvement. However, when testing with different numbers, K=5 resulted in worse predictions than just using 1NN. Let's plot the errors with different values of K:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "BM = []\n",
    "MB = []\n",
    "T = []\n",
    "\n",
    "def generate_errors(K):\n",
    "    pred = distRDD.groupByKey()\\\n",
    "            .mapValues(lambda vec: sorted(vec, key=lambda x: x[0])[:K])\\\n",
    "            .mapValues(lambda vec: [x[1] for x in vec])\\\n",
    "            .mapValues(lambda vec: Counter(vec).most_common(1)[0][0])\\\n",
    "            .values()\\\n",
    "            .map(lambda x: (x,1))\\\n",
    "            .reduceByKey(lambda val1, val2: val1 + val2)\\\n",
    "            .sortByKey()\\\n",
    "            .values()\\\n",
    "            .collect()\n",
    "            \n",
    "    BM.append(pred[1])\n",
    "    MB.append(pred[2])\n",
    "    T.append(pred[1]+pred[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vecK = range(1,61)\n",
    "for i in vecK:\n",
    "    generate_errors(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python3 range() is an iterator: convert it to list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
