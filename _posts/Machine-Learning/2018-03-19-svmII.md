---
layout: single
title: "Support Vector Machines (Part II)"
date: 2018-03-23
categories: machine-learning
tags: machine-learning non-linear-modelling support-vector-machines
author_profile: false
mathjax: true
toc: true
---

## Introduction

In Part I we had a small taste of SVM, however we will show here where they shine the most, allowing more powerful and extended applications.

## Linear and non-linear modelling

Although *linear model* is a term mostly used in regression techniques, its meaning embraces a high number of different algorithms with one thing in common: its modelization can be written as a linear combination of the parameters (do not confuse with the samples $x_i$), which comprises a strictly monotonic function. 

<aside class="notice--info">
A monotonic function is one where the order is preserved, either increasing or decreasing:
$$f(x_1) >= f(x_2) if (x_1) > (x_2).$$
We say that a function is strictly monotonic if the strict inequality holds. 
</aside>

This can be formalised as:

$$y(\textbf{x},\beta) = g (  \beta_0 + \sum_{i=1}^d \beta_i x_i  ) = g(\beta^T\textbf{x}),$$

$$\textbf{x} = (1,x_1,\ldots,x_d)^T, \beta=(\beta_0,\beta_1,\ldots,\beta_d)^T.$$

For example, a polynomic function.

Thus, we can generalize and obtain non-linearity when the parameters do not go hand by hand with the predictors, but with a non-linear **Basis Function**: 

$$y(\textbf{x},\beta) = g (  \beta_0 + \sum_{j=1}^M \beta_j \phi_j(\textbf{x})  ) = g(\beta^T\phi(\textbf{x})),$$

$$\phi(\textbf{x}) = (1,\phi_1(\textbf{x}),\ldots,\phi_M(\textbf{x}))^T, \beta=(\beta_0,\beta_1,\ldots,\beta_d)^T.$$

## Kernel Functions

Recall how SVM performed poorly when data is not linearly separable, showed with the example of the Iris dataset. In the **Kernel functions** lies the bread and butter of the SVM's power.

The idea behind the *kernel method* is to apply the following statement: 

> $n+1$ point can always be linearly separated in $\mathbb{R}^n$.

Then we want to use a Basis Function so that data gets projected into a high-dimensional feature space $F$ and perform there the linear modelling.

$$ \Phi:X \rightarrow F $$

$$ \textbf{x} \mapsto (1,\phi_1(\textbf{x}),\ldots,\phi_M(\textbf{x})) $$

So far we have feature vectors $\vec{x}$ of dimension $d$ and a function $\Phi$ that maps them into a $M$-dimensional space, where we require M to be large, i.e $M \gg d$. Trying to solve a problem with such dimensionality issues would end up badly for the estimation of the parameters $\beta$. However, thanks to the **Representer Theorem** we can rather aim to solve the **dual** formulation of $y$:

$$ y(\textbf{x},\beta) = g(\beta_0 + \sum_{j=1}^M \beta_j\phi_j(\textbf{x})) \mapsto $$

$$ y(\textbf{x},\alpha) = g(\sum_{n=1}^N \alpha_n t_n \langle \Phi(x_n),\Phi(\textbf{x})\rangle_F) = $$

$$ g(\sum_{n=1}^N \alpha_n t_n k(x_n,\textbf{x})) $$

![png](/assets/images/machinelearning/svm/kernel_diagram.png)